{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Step Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_h error:  1.8503734797e-09\n"
     ]
    }
   ],
   "source": [
    "from code_base.rnn_layers import rnn_step_forward\n",
    "from code_base.layer_utils import rel_error\n",
    "import numpy as np\n",
    "\n",
    "N, D, H = 3, 10, 4\n",
    "x = np.linspace(-0.4, 0.7, num=N*D).reshape(N, D)\n",
    "prev_h = np.linspace(-0.2, 0.5, num=N*H).reshape(N, H)\n",
    "Wx = np.linspace(-0.1, 0.9, num=D*H).reshape(D, H)\n",
    "Wh = np.linspace(-0.3, 0.7, num=H*H).reshape(H, H)\n",
    "b = np.linspace(-0.2, 0.4, num=H)\n",
    "next_h, _ = rnn_step_forward(x, prev_h, Wx, Wh, b)\n",
    "expected_next_h = np.asarray([\n",
    "[-0.58172089, -0.50182032, -0.41232771, -0.31410098],\n",
    "[ 0.66854692, 0.79562378, 0.87755553, 0.92795967],\n",
    "[ 0.97934501, 0.99144213, 0.99646691, 0.99854353]])\n",
    "print('next_h error: ', rel_error(expected_next_h, next_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from code_base.rnn_layers import rnn_step_forward\n",
    "import numpy as np\n",
    "\n",
    "x_shape = (3, 874)\n",
    "Wx_shape = (874, 128)\n",
    "h_shape = (3, 128)\n",
    "Wh_shape = (128, 128)\n",
    "b_shape = (128,)\n",
    "x = np.loadtxt('./input_files/x.csv', delimiter=',')\n",
    "x = x.reshape(x_shape)\n",
    "Wx = np.loadtxt('./input_files/Wx.csv', delimiter=',')\n",
    "Wx = Wx.reshape(Wx_shape)\n",
    "prev_h = np.loadtxt('./input_files/prev_h.csv', delimiter=',')\n",
    "prev_h = prev_h.reshape(h_shape)\n",
    "Wh = np.loadtxt('./input_files/Wh.csv', delimiter=',')\n",
    "Wh = Wh.reshape(Wh_shape)\n",
    "b = np.loadtxt('./input_files/b.csv', delimiter=',')\n",
    "out, _ = rnn_step_forward(x, prev_h, Wx, Wh, b)\n",
    "np.savetxt('./output_files/rnn_step_forward_out.csv', out.ravel(), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Step Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  8.76422864039e-11\n",
      "dprev_h error:  9.98826196438e-11\n",
      "dWx error:  3.96721710693e-11\n",
      "dWh error:  1.3358060921e-10\n",
      "db error:  2.7847821432e-11\n"
     ]
    }
   ],
   "source": [
    "from code_base.rnn_layers import rnn_step_forward, rnn_step_backward\n",
    "from code_base.gradient_check import *\n",
    "from code_base.layer_utils import rel_error\n",
    "import numpy as np\n",
    "\n",
    "N, D, H = 4, 5, 6\n",
    "x = np.random.randn(N, D)\n",
    "h = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, H)\n",
    "Wh = np.random.randn(H, H)\n",
    "b = np.random.randn(H)\n",
    "out, cache = rnn_step_forward(x, h, Wx, Wh, b)\n",
    "dnext_h = np.random.randn(*out.shape)\n",
    "fx = lambda x: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fh = lambda prev_h: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fWx = lambda Wx: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fWh = lambda Wh: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fb = lambda b: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dnext_h)\n",
    "dprev_h_num = eval_numerical_gradient_array(fh, h, dnext_h)\n",
    "dWx_num = eval_numerical_gradient_array(fWx, Wx, dnext_h)\n",
    "dWh_num = eval_numerical_gradient_array(fWh, Wh, dnext_h)\n",
    "db_num = eval_numerical_gradient_array(fb, b, dnext_h)\n",
    "dx, dprev_h, dWx, dWh, db = rnn_step_backward(dnext_h, cache)\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dprev_h error: ', rel_error(dprev_h_num, dprev_h))\n",
    "print('dWx error: ', rel_error(dWx_num, dWx))\n",
    "print('dWh error: ', rel_error(dWh_num, dWh))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from code_base.rnn_layers import rnn_step_forward, rnn_step_backward\n",
    "import numpy as np\n",
    "\n",
    "x_shape = (3, 874)\n",
    "Wx_shape = (874, 128)\n",
    "h_shape = (3, 128)\n",
    "Wh_shape = (128, 128)\n",
    "b_shape = (128,)\n",
    "x = np.loadtxt('./input_files/x.csv', delimiter=',')\n",
    "x = x.reshape(x_shape)\n",
    "Wx = np.loadtxt('./input_files/Wx.csv', delimiter=',')\n",
    "Wx = Wx.reshape(Wx_shape)\n",
    "prev_h = np.loadtxt('./input_files/prev_h.csv', delimiter=',')\n",
    "prev_h = prev_h.reshape(h_shape)\n",
    "Wh = np.loadtxt('./input_files/Wh.csv', delimiter=',')\n",
    "Wh = Wh.reshape(Wh_shape)\n",
    "b = np.loadtxt('./input_files/b.csv', delimiter=',')\n",
    "out, cache = rnn_step_forward(x, prev_h, Wx, Wh, b)\n",
    "dhout = np.loadtxt('./input_files/dho.csv', delimiter=',')\n",
    "dx, dh, dWx, dWh, db = rnn_step_backward(dhout, cache)\n",
    "np.savetxt('./output_files/rnn_step_backward_out_dx.csv', dx.ravel(), delimiter=',')\n",
    "np.savetxt('./output_files/rnn_step_backward_out_dh.csv', dh.ravel(), delimiter=',')\n",
    "np.savetxt('./output_files/rnn_step_backward_out_dwx.csv', dWx.ravel(), delimiter=',')\n",
    "np.savetxt('./output_files/rnn_step_backward_out_dwh.csv', dWh.ravel(), delimiter=',')\n",
    "np.savetxt('./output_files/rnn_step_backward_out_db.csv', db.ravel(), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h error:  1.04522949367e-08\n"
     ]
    }
   ],
   "source": [
    "from code_base.rnn_layers import rnn_forward\n",
    "from code_base.layer_utils import *\n",
    "import numpy as np\n",
    "\n",
    "N, T, D, H = 2, 3, 4, 5\n",
    "x = np.linspace(-0.1, 0.3, num=N*T*D).reshape(N, T, D)\n",
    "h0 = np.linspace(-0.3, 0.1, num=N*H).reshape(N, H)\n",
    "Wx = np.linspace(-0.2, 0.4, num=D*H).reshape(D, H)\n",
    "Wh = np.linspace(-0.4, 0.1, num=H*H).reshape(H, H)\n",
    "b = np.linspace(-0.7, 0.1, num=H)\n",
    "h, _ = rnn_forward(x, h0, Wx, Wh, b)\n",
    "expected_h = np.asarray([\n",
    "[[-0.42070749, -0.27279261, -0.11074945, 0.05740409, 0.22236251],\n",
    "[-0.39525808, -0.22554661, -0.0409454, 0.14649412, 0.32397316],\n",
    "[-0.42305111, -0.24223728, -0.04287027, 0.15997045, 0.35014525],],\n",
    "[[-0.55857474, -0.39065825, -0.19198182, 0.02378408, 0.23735671],\n",
    "[-0.27150199, -0.07088804, 0.13562939, 0.33099728, 0.50158768],\n",
    "[-0.51014825, -0.30524429, -0.06755202, 0.17806392, 0.40333043]]])\n",
    "print('h error: ', rel_error(expected_h, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from code_base.rnn_layers import rnn_forward\n",
    "import numpy as np\n",
    "\n",
    "x_all_shape = (3, 5, 874)\n",
    "Wx_shape = (874, 128)\n",
    "h_shape = (3, 128)\n",
    "Wh_shape = (128, 128)\n",
    "b_shape = (128,)\n",
    "x_all = np.loadtxt('./input_files/x_all.csv', delimiter=',')\n",
    "x_all = x_all.reshape(x_all_shape)\n",
    "Wx = np.loadtxt('./input_files/Wx.csv', delimiter=',')\n",
    "Wx = Wx.reshape(Wx_shape)\n",
    "h0 = np.loadtxt('./input_files/prev_h.csv', delimiter=',')\n",
    "h0 = prev_h.reshape(h_shape)\n",
    "Wh = np.loadtxt('./input_files/Wh.csv', delimiter=',')\n",
    "Wh = Wh.reshape(Wh_shape)\n",
    "b = np.loadtxt('./input_files/b.csv', delimiter=',')\n",
    "out, _ = rnn_forward(x_all, h0, Wx, Wh, b)\n",
    "np.savetxt('./output_files/rnn_forward_out.csv', out.ravel(), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  1.03276117629e-10\n",
      "dh0 error:  9.83060124999e-11\n",
      "dWx error:  2.28019753333e-10\n",
      "dWh error:  2.67111432307e-10\n",
      "db error:  3.90152423632e-10\n"
     ]
    }
   ],
   "source": [
    "from code_base.rnn_layers import rnn_forward, rnn_backward\n",
    "from code_base.gradient_check import *\n",
    "from code_base.layer_utils import *\n",
    "import numpy as np\n",
    "\n",
    "N, D, T, H = 2, 3, 10, 5\n",
    "x = np.random.randn(N, T, D)\n",
    "h0 = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, H)\n",
    "Wh = np.random.randn(H, H)\n",
    "b = np.random.randn(H)\n",
    "out, cache = rnn_forward(x, h0, Wx, Wh, b)\n",
    "dout = np.random.randn(*out.shape)\n",
    "dx, dh0, dWx, dWh, db = rnn_backward(dout, cache)\n",
    "fx = lambda x: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fh0 = lambda h0: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fWx = lambda Wx: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fWh = lambda Wh: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fb = lambda b: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "dh0_num = eval_numerical_gradient_array(fh0, h0, dout)\n",
    "dWx_num = eval_numerical_gradient_array(fWx, Wx, dout)\n",
    "dWh_num = eval_numerical_gradient_array(fWh, Wh, dout)\n",
    "db_num = eval_numerical_gradient_array(fb, b, dout)\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dh0 error: ', rel_error(dh0_num, dh0))\n",
    "print('dWx error: ', rel_error(dWx_num, dWx))\n",
    "print('dWh error: ', rel_error(dWh_num, dWh))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from code_base.rnn_layers import rnn_forward, rnn_backward\n",
    "import numpy as np\n",
    "\n",
    "x_all_shape = (3, 5, 874)\n",
    "Wx_shape = (874, 128)\n",
    "h_shape = (3, 128)\n",
    "Wh_shape = (128, 128)\n",
    "b_shape = (128,)\n",
    "dh_all_shape = (3, 5, 128)\n",
    "x_all = np.loadtxt('./input_files/x_all.csv', delimiter=',')\n",
    "x_all = x_all.reshape(x_all_shape)\n",
    "Wx = np.loadtxt('./input_files/Wx.csv', delimiter=',')\n",
    "Wx = Wx.reshape(Wx_shape)\n",
    "h0 = np.loadtxt('./input_files/prev_h.csv', delimiter=',')\n",
    "h0 = h0.reshape(h_shape)\n",
    "Wh = np.loadtxt('./input_files/Wh.csv', delimiter=',')\n",
    "Wh = Wh.reshape(Wh_shape)\n",
    "b = np.loadtxt('./input_files/b.csv', delimiter=',')\n",
    "out, cache = rnn_forward(x_all, h0, Wx, Wh, b)\n",
    "dhout = np.loadtxt('./input_files/dho_all.csv', delimiter=',')\n",
    "dhout = dhout.reshape(dh_all_shape)\n",
    "dx_all, dh0, dWx, dWh, db = rnn_backward(dhout, cache)\n",
    "np.savetxt('./output_files/rnn_backward_out_dx.csv', dx_all.ravel(), delimiter=',')\n",
    "np.savetxt('./output_files/rnn_backward_out_dh0.csv', dh0.ravel(), delimiter=',')\n",
    "np.savetxt('./output_files/rnn_backward_out_dwx.csv', dWx.ravel(), delimiter=',')\n",
    "np.savetxt('./output_files/rnn_backward_out_dwh.csv', dWh.ravel(), delimiter=',')\n",
    "np.savetxt('./output_files/rnn_backward_out_db.csv', db.ravel(), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Temporal Bi-directional Concatenation Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from code_base.rnn_layers import bidirectional_rnn_concatenate_forward\n",
    "from code_base.layer_utils import *\n",
    "import numpy as np\n",
    "\n",
    "N, T, H = 2, 4, 3\n",
    "h = np.linspace(-0.5, 0, num=N*T*H).reshape(N, T, H)\n",
    "hr = np.linspace(0, 0.5, num=N*T*H).reshape(N, T, H)\n",
    "mask = np.ones((N,T))\n",
    "mask[0][3] = 0 # length of s1 is 3\n",
    "mask[1][2] = mask[1][3] = 0 # length of s2 is 2\n",
    "ho, _ = bidirectional_rnn_concatenate_forward(h, hr, mask)\n",
    "expected_ho = np.array([[\n",
    "[-0.5, -0.47826087, -0.45652174, 0.13043478, 0.15217391, 0.17391304],\n",
    "[-0.43478261, -0.41304348, -0.39130435, 0.06521739, 0.08695652, 0.10869565],\n",
    "[-0.36956522, -0.34782609, -0.32608696, 0., 0.02173913, 0.04347826],\n",
    "[0., 0., 0., 0., 0., 0.]],\n",
    "[[-0.23913043, -0.2173913 , -0.19565217, 0.32608696, 0.34782609, 0.36956522],\n",
    "[-0.17391304, -0.15217391, -0.13043478, 0.26086957, 0.2826087, 0.30434783],\n",
    "[0., 0., 0., 0., 0., 0.],\n",
    "[0., 0., 0., 0., 0., 0.]]])\n",
    "print('ho error: ', rel_error(expected_ho, ho, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from code_base.rnn_layers import bidirectional_rnn_concatenate_forward\n",
    "from code_base.gradient_check import *\n",
    "import numpy as np\n",
    "\n",
    "h_shape = (3, 5, 128)\n",
    "mask_shape = (3, 5)\n",
    "h = np.loadtxt('./input_files/h_all.csv', delimiter=',')\n",
    "h = h.reshape(h_shape)\n",
    "hr = np.loadtxt('./input_files/h_all_r.csv', delimiter=',')\n",
    "hr = hr.reshape(h_shape)\n",
    "mask = np.loadtxt('./input_files/mask.csv', delimiter=',')\n",
    "mask = mask.reshape(mask_shape)\n",
    "hout, _ = bidirectional_rnn_concatenate_forward(h, hr, mask)\n",
    "np.savetxt('./output_files/bidirectional_rnn_concatenate_forward_out.csv', hout.ravel(), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Temporal Bi-directional Concatenation Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from code_base.rnn_layers import bidirectional_rnn_concatenate_forward, bidirectional_rnn_concatenate_backward\n",
    "from code_base.layer_utils import *\n",
    "from code_base.gradient_check import *\n",
    "import numpy as np\n",
    "\n",
    "N, T, H = 2, 4, 3\n",
    "h = np.linspace(-0.5, 0, num=N*T*H).reshape(N, T, H)\n",
    "hr = np.linspace(0, 0.5, num=N*T*H).reshape(N, T, H)\n",
    "mask = np.ones((N,T))\n",
    "mask[0][3] = 0 # length of s1 is 3\n",
    "mask[1][2] = mask[1][3] = 0 # length of s2 is 2\n",
    "ho, cache = bidirectional_rnn_concatenate_forward(h, hr, mask)\n",
    "dho = np.linspace(0., 0.5, num=N*T*2*H).reshape(N, T, 2*H)\n",
    "dh, dhr = bidirectional_rnn_concatenate_backward(dho, cache)\n",
    "fh = lambda h: bidirectional_rnn_concatenate_forward(h, hr, mask)[0]\n",
    "fhr = lambda hr: bidirectional_rnn_concatenate_forward(h, hr, mask)[0]\n",
    "dh_num = eval_numerical_gradient_array(fh, h, dho)\n",
    "dhr_num = eval_numerical_gradient_array(fhr, hr, dho)\n",
    "print('dh error: ', rel_error(dh_num, dh, mask))\n",
    "print('dhr error: ', rel_error(dhr_num, dhr, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from code_base.rnn_layers import bidirectional_rnn_concatenate_forward, bidirectional_rnn_concatenate_backward\n",
    "import numpy as np\n",
    "\n",
    "h_shape = (3, 5, 128)\n",
    "mask_shape = (3, 5)\n",
    "h = np.loadtxt('./input_files/h_all.csv', delimiter=',')\n",
    "h = h.reshape(h_shape)\n",
    "hr = np.loadtxt('./input_files/h_all_r.csv', delimiter=',')\n",
    "hr = hr.reshape(h_shape)\n",
    "mask = np.loadtxt('./input_files/mask.csv', delimiter=',')\n",
    "mask = mask.reshape(mask_shape)\n",
    "hout, cache = bidirectional_rnn_concatenate_forward(h, hr, mask)\n",
    "dhout = np.loadtxt('./input_files/dhc_all.csv', delimiter=',')\n",
    "dhout = dhout.reshape(3, 5, 256)\n",
    "dh, dhr = bidirectional_rnn_concatenate_backward(dhout, cache)\n",
    "np.savetxt('./output_files/bidirectional_rnn_concatenate_backward_out_h.csv', dh.ravel(), delimiter=',')\n",
    "np.savetxt('./output_files/bidirectional_rnn_concatenate_backward_out_hr.csv', dhr.ravel(), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for Sentiment Analysis - Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  2.99619226823\n",
      "expected loss:  2.99619226823\n",
      "difference:  1.38733469157e-12\n"
     ]
    }
   ],
   "source": [
    "from code_base.classifiers.rnn import *\n",
    "# If you do brnn, please import from code_base.classifiers.brnn instead\n",
    "import numpy as np\n",
    "\n",
    "N, H, A, O = 2, 6, 5, 2\n",
    "word_to_idx = { 'awesome': 0, 'reading':1, 'pretty': 2, 'dog': 3, 'movie': 4,\n",
    "                'liked': 5, 'most': 6, 'admired': 7, 'bad': 8, 'fucking': 9}\n",
    "V = len(word_to_idx)\n",
    "T = 4\n",
    "model = SentimentAnalysisRNN(word_to_idx,\n",
    "    hidden_dim=H,\n",
    "    fc_dim=A,\n",
    "    output_dim=O,\n",
    "    cell_type='rnn',\n",
    "    dtype=np.float64)\n",
    "# Set all model parameters to fixed values\n",
    "for k, v in model.params.items():\n",
    "    model.params[k] = np.linspace(-1.4, 1.3, num=v.size).reshape(*v.shape)\n",
    "labels = np.array([1, 0], dtype=np.int32)\n",
    "wordvecs = np.zeros((N, T, V))\n",
    "wordvecs[0, 0, 0] = wordvecs[0, 1, 5] = wordvecs[0, 2, 2] = wordvecs[0, 3, 7] = 1\n",
    "wordvecs[1, 0, 4] = wordvecs[1, 1, 8] = wordvecs[1, 2, 5] = 1\n",
    "mask = np.ones((N, T))\n",
    "mask[1, 3] = 0\n",
    "loss, grads = model.loss(wordvecs, labels, mask)\n",
    "expected_loss = 2.99619226823\n",
    "# For brnn, the expected_loss should be 2.9577205234\n",
    "print('loss: ', loss)\n",
    "print('expected loss: ', expected_loss)\n",
    "print('difference: ', abs(loss - expected_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for Sentiment Analysis - Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_a relative error: 2.470576e-09\n",
      "W_fc relative error: 2.570464e-10\n",
      "Wh relative error: 1.117105e-07\n",
      "Wx relative error: 4.387683e-08\n",
      "b relative error: 9.065656e-09\n",
      "b_a relative error: 1.536972e-09\n",
      "b_fc relative error: 4.681212e-12\n"
     ]
    }
   ],
   "source": [
    "from code_base.classifiers.rnn import *\n",
    "# If you do brnn, please import from code_base.classifiers.brnn instead\n",
    "from code_base.gradient_check import *\n",
    "from code_base.layer_utils import rel_error\n",
    "import numpy as np\n",
    "\n",
    "N, T, H, A, O = 2, 4, 6, 5, 2\n",
    "word_to_idx = {'awesome': 0, 'reading':1, 'pretty': 2, 'dog': 3, 'movie': 4,\n",
    "                'liked': 5, 'most': 6, 'admired': 7, 'bad': 8, 'fucking': 9}\n",
    "V = len(word_to_idx)\n",
    "labels = np.array([1, 0], dtype=np.int32)\n",
    "wordvecs = np.zeros((N, T, V))\n",
    "wordvecs[0, 0, 0] = wordvecs[0, 1, 5] = wordvecs[0, 2, 2] = wordvecs[0, 3, 7] = 1\n",
    "wordvecs[1, 0, 4] = wordvecs[1, 1, 8] = wordvecs[1, 2, 5] = 1\n",
    "mask = np.ones((N, T))\n",
    "mask[1, 3] = 0\n",
    "model = SentimentAnalysisRNN(word_to_idx,\n",
    "    hidden_dim=H,\n",
    "    fc_dim=A,\n",
    "    output_dim=O,\n",
    "    cell_type='rnn',\n",
    "    dtype=np.float64,\n",
    ")\n",
    "loss, grads = model.loss(wordvecs, labels, mask)\n",
    "for param_name in sorted(grads):\n",
    "    f = lambda _: model.loss(wordvecs, labels, mask)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, model.params[param_name],\n",
    "verbose=False, h=1e-6)\n",
    "    e = rel_error(param_grad_num, grads[param_name])\n",
    "    print('%s relative error: %e' % (param_name, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/Inference on Small Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/waiyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "(Iteration 1 / 100) loss: 0.315483\n",
      "(Iteration 11 / 100) loss: 0.277323\n",
      "(Iteration 21 / 100) loss: 0.243838\n",
      "(Iteration 31 / 100) loss: 0.212798\n",
      "(Iteration 41 / 100) loss: 0.183810\n",
      "(Iteration 51 / 100) loss: 0.157254\n",
      "(Iteration 61 / 100) loss: 0.133639\n",
      "(Iteration 71 / 100) loss: 0.113249\n",
      "(Iteration 81 / 100) loss: 0.096055\n",
      "(Iteration 91 / 100) loss: 0.081789\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1132042b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from code_base.sentiment_analysis_solver import *\n",
    "from code_base.classifiers.rnn import *\n",
    "# If you do brnn, please import from code_base.classifiers.brnn instead\n",
    "from code_base.data_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "download_corpus()\n",
    "small_data = load_data('code_base/datasets/train.csv', sample=True)\n",
    "small_rnn_model = SentimentAnalysisRNN(\n",
    "    cell_type='rnn',\n",
    "    word_to_idx=load_dictionary('code_base/datasets/dictionary.csv')\n",
    ")\n",
    "small_rnn_solver = SentimentAnalysisSolver(small_rnn_model,\n",
    "    small_data,\n",
    "    update_rule='sgd',\n",
    "    num_epochs=100,\n",
    "    batch_size=100,\n",
    "    optim_config={\n",
    "        'learning_rate': 5e-3,\n",
    "    },\n",
    "    lr_decay=1.0,\n",
    "    verbose=True,\n",
    "    print_every=10,\n",
    ")\n",
    "small_rnn_solver.train()\n",
    "\n",
    "# we will use the same batch of training data for inference\n",
    "# this is just to let you know the procedure of inference\n",
    "preds = small_rnn_solver.test(split='train')\n",
    "np.savetxt('./output_files/rnn_prediction_prob.csv', preds.ravel(), delimiter=',')\n",
    "# If you do brnn, please save result to ./output_files/brnn_prediction_prob.csv\n",
    "\n",
    "# Plot the training losses\n",
    "plt.plot(small_rnn_solver.loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
