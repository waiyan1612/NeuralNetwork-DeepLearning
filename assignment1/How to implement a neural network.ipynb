{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/waiyan/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Tutorial from http://peterroelants.github.io/posts/neural_network_implementation_part05/\n",
    "\n",
    "# Python imports\n",
    "import numpy as np # Matrix and vector computation package\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "# Allow matplotlib to plot inside this notebook\n",
    "%matplotlib inline\n",
    "# Set the seed of the numpy random number generator so that the tutorial is reproducable\n",
    "np.random.seed(seed=1)\n",
    "from sklearn import datasets, cross_validation, metrics # data and evaluation utils\n",
    "from matplotlib.colors import colorConverter, ListedColormap # some plotting functions\n",
    "import itertools\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the data from scikit-learn.\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Load the targets.\n",
    "# Note that the targets are stored as digits, these need to be \n",
    "#  converted to one-hot-encoding for the output sofmax layer.\n",
    "T = np.zeros((digits.target.shape[0],10))\n",
    "T[np.arange(len(T)), digits.target] += 1\n",
    "\n",
    "# Divide the data into a train and test set.\n",
    "X_train, X_test, T_train, T_test = cross_validation.train_test_split(\n",
    "    digits.data, T, test_size=0.4)\n",
    "# Divide the test set into a validation set and final test set.\n",
    "X_validation, X_test, T_validation, T_test = cross_validation.train_test_split(\n",
    "    X_test, T_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzEAAABxCAYAAADlJi1xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/CUgVSbiBAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105764e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " # Plot an example of each image.\n",
    "fig = plt.figure(figsize=(10, 1), dpi=100)\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(1,10,i+1)\n",
    "    ax.matshow(digits.images[i], cmap='binary') \n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the non-linear functions used\n",
    "def logistic(z): \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_deriv(y):  # Derivative of logistic function\n",
    "    return np.multiply(y, (1 - y))\n",
    "    \n",
    "def softmax(z): \n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    " # Define the layers used in this model\n",
    "class Layer(object):\n",
    "    \"\"\"Base class for the different layers.\n",
    "    Defines base methods and documentation of methods.\"\"\"\n",
    "    \n",
    "    def get_params_iter(self):\n",
    "        \"\"\"Return an iterator over the parameters (if any).\n",
    "        The iterator has the same order as get_params_grad.\n",
    "        The elements returned by the iterator are editable in-place.\"\"\"\n",
    "        return []\n",
    "    \n",
    "    def get_params_grad(self, X, output_grad):\n",
    "        \"\"\"Return a list of gradients over the parameters.\n",
    "        The list has the same order as the get_params_iter iterator.\n",
    "        X is the input.\n",
    "        output_grad is the gradient at the output of this layer.\n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step linear transformation.\n",
    "        X is the input.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad=None, T=None):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\n",
    "        Y is the pre-computed output of this layer (not needed in this case).\n",
    "        output_grad is the gradient at the output of this layer \n",
    "         (gradient at input of next layer).\n",
    "        Output layer uses targets T to compute the gradient based on the \n",
    "         output error instead of output_grad\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearLayer(Layer):\n",
    "    \"\"\"The linear layer performs a linear transformation to its input.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_in, n_out):\n",
    "        \"\"\"Initialize hidden layer parameters.\n",
    "        n_in is the number of input variables.\n",
    "        n_out is the number of output variables.\"\"\"\n",
    "        self.W = np.random.randn(n_in, n_out) * 0.1\n",
    "        self.b = np.zeros(n_out)\n",
    "        \n",
    "    def get_params_iter(self):\n",
    "        \"\"\"Return an iterator over the parameters.\"\"\"\n",
    "        return itertools.chain(np.nditer(self.W, op_flags=['readwrite']),\n",
    "                               np.nditer(self.b, op_flags=['readwrite']))\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step linear transformation.\"\"\"\n",
    "        return X.dot(self.W) + self.b\n",
    "        \n",
    "    def get_params_grad(self, X, output_grad):\n",
    "        \"\"\"Return a list of gradients over the parameters.\"\"\"\n",
    "        JW = X.T.dot(output_grad)\n",
    "        Jb = np.sum(output_grad, axis=0)\n",
    "        return [g for g in itertools.chain(np.nditer(JW), np.nditer(Jb))]\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "        return output_grad.dot(self.W.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticLayer(Layer):\n",
    "    \"\"\"The logistic layer applies the logistic function to its inputs.\"\"\"\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "        return logistic(X)\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "        return np.multiply(logistic_deriv(Y), output_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftmaxOutputLayer(Layer):\n",
    "    \"\"\"The softmax output layer computes the classification propabilities at the output.\"\"\"\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "        return softmax(X)\n",
    "    \n",
    "    def get_input_grad(self, Y, T):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "        return (Y - T) / Y.shape[0]\n",
    "    \n",
    "    def get_cost(self, Y, T):\n",
    "        \"\"\"Return the cost at the output of this output layer.\"\"\"\n",
    "        return - np.multiply(T, np.log(Y)).sum() / Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a sample model to be trained on the data\n",
    "hidden_neurons_1 = 20  # Number of neurons in the first hidden-layer\n",
    "hidden_neurons_2 = 20  # Number of neurons in the second hidden-layer\n",
    "# Create the model\n",
    "layers = [] # Define a list of layers\n",
    "# Add first hidden layer\n",
    "layers.append(LinearLayer(X_train.shape[1], hidden_neurons_1))\n",
    "layers.append(LogisticLayer())\n",
    "# Add second hidden layer\n",
    "layers.append(LinearLayer(hidden_neurons_1, hidden_neurons_2))\n",
    "layers.append(LogisticLayer())\n",
    "# Add output layer\n",
    "layers.append(LinearLayer(hidden_neurons_2, T_train.shape[1]))\n",
    "layers.append(SoftmaxOutputLayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the forward propagation step as a method.\n",
    "def forward_step(input_samples, layers):\n",
    "    \"\"\"\n",
    "    Compute and return the forward activation of each layer in layers.\n",
    "    Input:\n",
    "        input_samples: A matrix of input samples (each row is an input vector)\n",
    "        layers: A list of Layers\n",
    "    Output:\n",
    "        A list of activations where the activation at each index i+1 corresponds to\n",
    "        the activation of layer i in layers. activations[0] contains the input samples.  \n",
    "    \"\"\"\n",
    "    activations = [input_samples] # List of layer activations\n",
    "    # Compute the forward activations for each layer starting from the first\n",
    "    X = input_samples\n",
    "    for layer in layers:\n",
    "        Y = layer.get_output(X)  # Get the output of the current layer\n",
    "        activations.append(Y)  # Store the output for future processing\n",
    "        X = activations[-1]  # Set the current input as the activations of the previous layer\n",
    "    return activations  # Return the activations of each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the backward propagation step as a method\n",
    "def backward_step(activations, targets, layers):\n",
    "    \"\"\"\n",
    "    Perform the backpropagation step over all the layers and return the parameter gradients.\n",
    "    Input:\n",
    "        activations: A list of forward step activations where the activation at \n",
    "            each index i+1 corresponds to the activation of layer i in layers. \n",
    "            activations[0] contains the input samples. \n",
    "        targets: The output targets of the output layer.\n",
    "        layers: A list of Layers corresponding that generated the outputs in activations.\n",
    "    Output:\n",
    "        A list of parameter gradients where the gradients at each index corresponds to\n",
    "        the parameters gradients of the layer at the same index in layers. \n",
    "    \"\"\"\n",
    "    param_grads = collections.deque()  # List of parameter gradients for each layer\n",
    "    output_grad = None  # The error gradient at the output of the current layer\n",
    "    # Propagate the error backwards through all the layers.\n",
    "    #  Use reversed to iterate backwards over the list of layers.\n",
    "    for layer in reversed(layers):   \n",
    "        Y = activations.pop()  # Get the activations of the last layer on the stack\n",
    "        # Compute the error at the output layer.\n",
    "        # The output layer error is calculated different then hidden layer error.\n",
    "        if output_grad is None:\n",
    "            input_grad = layer.get_input_grad(Y, targets)\n",
    "        else:  # output_grad is not None (layer is not output layer)\n",
    "            input_grad = layer.get_input_grad(Y, output_grad)\n",
    "        # Get the input of this layer (activations of the previous layer)\n",
    "        X = activations[-1]\n",
    "        # Compute the layer parameter gradients used to update the parameters\n",
    "        grads = layer.get_params_grad(X, output_grad)\n",
    "        param_grads.appendleft(grads)\n",
    "        # Compute gradient at output of previous layer (input of current layer):\n",
    "        output_grad = input_grad\n",
    "    return list(param_grads)  # Return the parameter gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No gradient errors found\n"
     ]
    }
   ],
   "source": [
    "# Perform gradient checking\n",
    "nb_samples_gradientcheck = 10 # Test the gradients on a subset of the data\n",
    "X_temp = X_train[0:nb_samples_gradientcheck,:]\n",
    "T_temp = T_train[0:nb_samples_gradientcheck,:]\n",
    "# Get the parameter gradients with backpropagation\n",
    "activations = forward_step(X_temp, layers)\n",
    "param_grads = backward_step(activations, T_temp, layers)\n",
    "\n",
    "# Set the small change to compute the numerical gradient\n",
    "eps = 0.0001\n",
    "# Compute the numerical gradients of the parameters in all layers.\n",
    "for idx in range(len(layers)):\n",
    "    layer = layers[idx]\n",
    "    layer_backprop_grads = param_grads[idx]\n",
    "    # Compute the numerical gradient for each parameter in the layer\n",
    "    for p_idx, param in enumerate(layer.get_params_iter()):\n",
    "        grad_backprop = layer_backprop_grads[p_idx]\n",
    "        # + eps\n",
    "        param += eps\n",
    "        plus_cost = layers[-1].get_cost(forward_step(X_temp, layers)[-1], T_temp)\n",
    "        # - eps\n",
    "        param -= 2 * eps\n",
    "        min_cost = layers[-1].get_cost(forward_step(X_temp, layers)[-1], T_temp)\n",
    "        # reset param value\n",
    "        param += eps\n",
    "        # calculate numerical gradient\n",
    "        grad_num = (plus_cost - min_cost)/(2*eps)\n",
    "        # Raise error if the numerical grade is not close to the backprop gradient\n",
    "        if not np.isclose(grad_num, grad_backprop):\n",
    "            raise ValueError('Numerical gradient of {:.6f} is not close to the backpropagation gradient of {:.6f}!'.format(float(grad_num), float(grad_backprop)))\n",
    "print('No gradient errors found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the minibatches\n",
    "batch_size = 25  # Approximately 25 samples per batch\n",
    "nb_of_batches = X_train.shape[0] / batch_size  # Number of batches\n",
    "# Create batches (X,Y) from the training set\n",
    "XT_batches = list(zip(\n",
    "    np.array_split(X_train, nb_of_batches, axis=0),  # X samples\n",
    "    np.array_split(T_train, nb_of_batches, axis=0)))  # Y targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a method to update the parameters\n",
    "def update_params(layers, param_grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Function to update the parameters of the given layers with the given gradients\n",
    "    by gradient descent with the given learning rate.\n",
    "    \"\"\"\n",
    "    for layer, layer_backprop_grads in list(zip(layers, param_grads)):\n",
    "        for param, grad in zip(layer.get_params_iter(), layer_backprop_grads):\n",
    "            # The parameter returned by the iterator point to the memory space of\n",
    "            #  the original layer and can thus be modified inplace.\n",
    "            param -= learning_rate * grad  # Update each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    " # Perform backpropagation\n",
    "# initalize some lists to store the cost for future analysis        \n",
    "minibatch_costs = []\n",
    "training_costs = []\n",
    "validation_costs = []\n",
    "\n",
    "max_nb_of_iterations = 300  # Train for a maximum of 300 iterations\n",
    "learning_rate = 0.1  # Gradient descent learning rate\n",
    "\n",
    "# Train for the maximum number of iterations\n",
    "for iteration in range(max_nb_of_iterations):\n",
    "    \n",
    "    for X, T in XT_batches:  # For each minibatch sub-iteration\n",
    "        activations = forward_step(X, layers)  # Get the activations\n",
    "        minibatch_cost = layers[-1].get_cost(activations[-1], T)  # Get cost\n",
    "        minibatch_costs.append(minibatch_cost)\n",
    "        param_grads = backward_step(activations, T, layers)  # Get the gradients\n",
    "        update_params(layers, param_grads, learning_rate)  # Update the parameters\n",
    "    \n",
    "    # Get full training cost for future analysis (plots)\n",
    "    activations = forward_step(X_train, layers)\n",
    "    train_cost = layers[-1].get_cost(activations[-1], T_train)\n",
    "    training_costs.append(train_cost)\n",
    "    \n",
    "    # Get full validation cost\n",
    "    activations = forward_step(X_validation, layers)\n",
    "    validation_cost = layers[-1].get_cost(activations[-1], T_validation)\n",
    "    validation_costs.append(validation_cost)\n",
    "    if len(validation_costs) > 3:\n",
    "        # Stop training if the cost on the validation set doesn't decrease\n",
    "        #  for 3 iterations\n",
    "        if validation_costs[-1] >= validation_costs[-2] >= validation_costs[-3]:\n",
    "            break\n",
    "\n",
    "nb_of_iterations = iteration + 1  # The number of iterations that have been executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxpgy+X+ZhvY4JQr2ugAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10577b898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    " # Plot the minibatch, full training set, and validation costs\n",
    "minibatch_x_inds = np.linspace(0, nb_of_iterations, num=nb_of_iterations*nb_of_batches)\n",
    "iteration_x_inds = np.linspace(1, nb_of_iterations, num=nb_of_iterations)\n",
    "# Plot the cost over the iterations\n",
    "#plt.plot(minibatch_x_inds, minibatch_costs, 'k-', linewidth=0.5, label='cost minibatches')\n",
    "plt.plot(iteration_x_inds, training_costs, 'r-', linewidth=2, label='cost full training set')\n",
    "plt.plot(iteration_x_inds, validation_costs, 'b-', linewidth=3, label='cost validation set')\n",
    "# Add labels to the plot\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('$\\\\xi$', fontsize=15)\n",
    "plt.title('Decrease of cost over backprop iteration')\n",
    "plt.legend()\n",
    "x1,x2,y1,y2 = plt.axis()\n",
    "plt.axis((0,nb_of_iterations,0,2.5))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set is 0.96\n"
     ]
    }
   ],
   "source": [
    "# Get results of test data\n",
    "y_true = np.argmax(T_test, axis=1)  # Get the target outputs\n",
    "activations = forward_step(X_test, layers)  # Get activation of test samples\n",
    "y_pred = np.argmax(activations[-1], axis=1)  # Get the predictions made by the network\n",
    "test_accuracy = metrics.accuracy_score(y_true, y_pred)  # Test set accuracy\n",
    "print('The accuracy on the test set is {:.2f}'.format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
