{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import numpy as np # Matrix and vector computation package\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "# Allow matplotlib to plot inside this notebook\n",
    "%matplotlib inline\n",
    "# Set the seed of the numpy random number generator so that the tutorial is reproducable\n",
    "np.random.seed(seed=1)\n",
    "from sklearn import datasets, metrics # data and evaluation utils\n",
    "from matplotlib.colors import colorConverter, ListedColormap # some plotting functions\n",
    "import itertools\n",
    "import collections\n",
    "import math\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_weights(filename, weights):\n",
    "    with open(filename, \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        for w in weights:\n",
    "            writer.writerows(w)\n",
    "\n",
    "# Define the non-linear functions used\n",
    "def logistic(z): \n",
    "    #return 1 / (1 + np.exp(-z))\n",
    "    #return np.maximum(0, z)\n",
    "    return z * (z > 0)\n",
    "\n",
    "def logistic_deriv(y):  # Derivative of logistic function\n",
    "    #return np.multiply(y, (1 - y))\n",
    "    return 1. * (y > 0)\n",
    "    \n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "    #return logistic(x)\n",
    "    #e_x = np.exp(x - np.max(x))\n",
    "    #a=  e_x / np.sum(e_x, axis=0, keepdims = True)\n",
    "    #return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the layers used in this model\n",
    "class Layer(object):\n",
    "    \"\"\"Base class for the different layers.\n",
    "    Defines base methods and documentation of methods.\"\"\"\n",
    "    \n",
    "    def get_layer_type(self):\n",
    "        return \"\"\n",
    "    \n",
    "    def get_params_iter(self):\n",
    "        \"\"\"Return an iterator over the parameters (if any).\n",
    "        The iterator has the same order as get_params_grad.\n",
    "        The elements returned by the iterator are editable in-place.\"\"\"\n",
    "        return []\n",
    "    \n",
    "    def get_params_grad(self, X, output_grad):\n",
    "        \"\"\"Return a list of gradients over the parameters.\n",
    "        The list has the same order as the get_params_iter iterator.\n",
    "        X is the input.\n",
    "        output_grad is the gradient at the output of this layer.\n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step linear transformation.\n",
    "        X is the input.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def getW(self):\n",
    "        return []\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad=None, T=None):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\n",
    "        Y is the pre-computed output of this layer (not needed in this case).\n",
    "        output_grad is the gradient at the output of this layer \n",
    "         (gradient at input of next layer).\n",
    "        Output layer uses targets T to compute the gradient based on the \n",
    "         output error instead of output_grad\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearLayer(Layer):\n",
    "    \"\"\"The linear layer performs a linear transformation to its input.\"\"\"\n",
    "    \n",
    "    def get_layer_type(self):\n",
    "        return \"linear\"\n",
    "    \n",
    "    def __init__(self, n_in, n_out):\n",
    "        \"\"\"Initialize hidden layer parameters.\n",
    "        n_in is the number of input variables.\n",
    "        n_out is the number of output variables.\"\"\"\n",
    "        self.W = np.random.randn(n_in, n_out) *0.1\n",
    "        self.b = np.zeros(n_out)\n",
    "\n",
    "        #self.b = n_out * 0.5\n",
    "        #self.W = np.random.randn(n_in, n_out).astype(np.float32) * np.sqrt(2.0/(n_in)) * 0.1\n",
    "        #self.b = np.zeros([n_out]).astype(np.float32)  * 0.1\n",
    "        #self.b = np.reshape(self.b,(n_out,1))\n",
    "        \n",
    "    def get_params_iter(self):\n",
    "        \"\"\"Return an iterator over the parameters.\"\"\"\n",
    "        return itertools.chain(np.nditer(self.W, op_flags=['readwrite']),\n",
    "                               np.nditer(self.b, op_flags=['readwrite']))\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step linear transformation.\"\"\"\n",
    "        return X.dot(self.W) + self.b\n",
    "        \n",
    "    def get_params_grad(self, X, output_grad):\n",
    "        \"\"\"Return a list of gradients over the parameters.\"\"\"\n",
    "        JW = X.T.dot(output_grad)\n",
    "        Jb = np.sum(output_grad, axis=0)\n",
    "        np.set_printoptions(suppress=True)\n",
    "        #print(\"JWshpe\"+ str(JW.T.shape))\n",
    "        #print(\"JW\"+ str(JW.T))\n",
    "        #print(\"JBshpe\"+ str(Jb.T.shape))\n",
    "        #print(\"JB\"+ str(Jb.T))\n",
    "        return [g for g in itertools.chain(np.nditer(JW), np.nditer(Jb))]\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "        return output_grad.dot(self.W.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticLayer(Layer):\n",
    "    \"\"\"The logistic layer applies the logistic function to its inputs.\"\"\"\n",
    "    \n",
    "    def get_layer_type(self):\n",
    "        return \"activation\"\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "        return logistic(X)\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "        return np.multiply(logistic_deriv(Y), output_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftmaxOutputLayer(Layer):\n",
    "    \"\"\"The softmax output layer computes the classification propabilities at the output.\"\"\"\n",
    "    \n",
    "    def get_layer_type(self):\n",
    "        return \"activation\"\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "        return softmax(X)\n",
    "    \n",
    "    def get_input_grad(self, Y, T):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "        return (Y - T) / Y.shape[0]\n",
    "    \n",
    "    def get_cost(self, Y, T):\n",
    "        \"\"\"Return the cost at the output of this output layer.\"\"\"\n",
    "        return - np.multiply(T, np.ma.log(Y)).sum() / Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_onehot_vec(data, shape):\n",
    "    Y = np.zeros(shape=shape, dtype=int)\n",
    "    Y[data, np.arange(data.size)] = 1\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9175, 4)\n"
     ]
    }
   ],
   "source": [
    "X = np.loadtxt(fname = \"Question2_123/x_train.csv\", dtype=int, delimiter=',', unpack=True)\n",
    "m = X.shape[1]\n",
    "\n",
    "split = math.ceil(0.7 * m)\n",
    "\n",
    "X_validate = X [:, split: ]\n",
    "X = X [:, : split]\n",
    "\n",
    "#X= [-1, 1, 1, 1, -1, -1, 1, -1, 1, 1, -1, -1, 1, 1]\n",
    "#X = np.reshape(X,  (14, 1))\n",
    "data = np.loadtxt(fname = \"Question2_123/y_train.csv\", dtype=int, delimiter=',')\n",
    "Y = to_onehot_vec(data, (4,m))\n",
    "\n",
    "Y_validate = Y [:, split: ]\n",
    "Y = Y [:, : split]\n",
    "\n",
    "X_test= np.loadtxt(fname = \"Question2_123/x_test.csv\", dtype=int, delimiter=',', unpack=True)\n",
    "\n",
    "\n",
    "data = np.loadtxt(fname = \"Question2_123/y_test.csv\", dtype=int, delimiter=',')\n",
    "Y_test = to_onehot_vec(data, (4, X_test.shape[1]))\n",
    "X= X.T\n",
    "X_validate = X_validate.T\n",
    "X_test = X_test.T\n",
    "Y = Y.T\n",
    "Y_validate = Y_validate.T\n",
    "Y_test = Y_test.T\n",
    "\n",
    "print(Y.shape)\n",
    "\n",
    "# Create the minibatches\n",
    "batch_size = 25\n",
    "no_of_batches = m / batch_size\n",
    "# Create batches (X,Y) from the training set\n",
    "batches = zip(\n",
    "    np.array_split(X, no_of_batches, axis=0),  # X samples\n",
    "    np.array_split(Y, no_of_batches, axis=0))  # Y targets\n",
    "\n",
    "biases = np.genfromtxt(fname = 'Question2_4/b/b-100-40-4.csv', delimiter=',')\n",
    "b1 = biases[0][:100]\n",
    "b2 = biases[1][:40]\n",
    "b3 = biases[2][:4]\n",
    "b1 = np.reshape(b1, (1, 100))\n",
    "b2 = np.reshape(b2, (1, 40))\n",
    "b3 = np.reshape(b3, (1, 4))\n",
    "\n",
    "weights = np.genfromtxt(fname = 'Question2_4/b/w-100-40-4.csv', delimiter=',')\n",
    "w1 = weights[:14,:100]\n",
    "w2 = weights[14:114,:40]\n",
    "w3 = weights[114:,:4]\n",
    "\n",
    "\n",
    "w1 = np.reshape(w1, (100, 14))\n",
    "w2 = np.reshape(w2, (40, 100))\n",
    "w3 = np.reshape(w3, (4, 40))\n",
    "ls_costs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a sample model to be trained on the data\n",
    "hidden_neurons_1 = 40  # Number of neurons in the first hidden-layer\n",
    "hidden_neurons_2 = 100  # Number of neurons in the second hidden-layer\n",
    "# Create the model\n",
    "layers = [] # Define a list of layers\n",
    "# Add first hidden layer\n",
    "#layers.append(LinearLayer(w1,b1))\n",
    "layers.append(LinearLayer(X.shape[1], hidden_neurons_1))\n",
    "layers.append(LogisticLayer())\n",
    "# Add second hidden layer\n",
    "#layers.append(LinearLayer(w2,b2))\n",
    "layers.append(LinearLayer(hidden_neurons_1, hidden_neurons_2))\n",
    "layers.append(LogisticLayer())\n",
    "# Add output layer\n",
    "#layers.append(LinearLayer(w3,b3))\n",
    "layers.append(LinearLayer(hidden_neurons_2, Y.shape[1]))\n",
    "layers.append(SoftmaxOutputLayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the forward propagation step as a method.\n",
    "def forward_step(input_samples, layers):\n",
    "    \"\"\"\n",
    "    Compute and return the forward activation of each layer in layers.\n",
    "    Input:\n",
    "        input_samples: A matrix of input samples (each row is an input vector)\n",
    "        layers: A list of Layers\n",
    "    Output:\n",
    "        A list of activations where the activation at each index i+1 corresponds to\n",
    "        the activation of layer i in layers. activations[0] contains the input samples.  \n",
    "    \"\"\"\n",
    "    activations = [input_samples] # List of layer activations\n",
    "    # Compute the forward activations for each layer starting from the first\n",
    "    X = input_samples\n",
    "    for layer in layers:\n",
    "        Y = layer.get_output(X)  # Get the output of the current layer\n",
    "        activations.append(Y)  # Store the output for future processing\n",
    "        X = activations[-1]  # Set the current input as the activations of the previous layer\n",
    "    return activations  # Return the activations of each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the backward propagation step as a method\n",
    "def backward_step(activations, targets, layers):\n",
    "    \"\"\"\n",
    "    Perform the backpropagation step over all the layers and return the parameter gradients.\n",
    "    Input:\n",
    "        activations: A list of forward step activations where the activation at \n",
    "            each index i+1 corresponds to the activation of layer i in layers. \n",
    "            activations[0] contains the input samples. \n",
    "        targets: The output targets of the output layer.\n",
    "        layers: A list of Layers corresponding that generated the outputs in activations.\n",
    "    Output:\n",
    "        A list of parameter gradients where the gradients at each index corresponds to\n",
    "        the parameters gradients of the layer at the same index in layers. \n",
    "    \"\"\"\n",
    "    param_grads = collections.deque()  # List of parameter gradients for each layer\n",
    "    output_grad = None  # The error gradient at the output of the current layer\n",
    "    # Propagate the error backwards through all the layers.\n",
    "    # Use reversed to iterate backwards over the list of layers.\n",
    "    for layer in reversed(layers):   \n",
    "        Y = activations.pop()  # Get the activations of the last layer on the stack\n",
    "        # Compute the error at the output layer.\n",
    "        # The output layer error is calculated different then hidden layer error.\n",
    "        if output_grad is None:\n",
    "            input_grad = layer.get_input_grad(Y, targets)\n",
    "        else:  # output_grad is not None (layer is not output layer)\n",
    "            input_grad = layer.get_input_grad(Y, output_grad)\n",
    "        # Get the input of this layer (activations of the previous layer)\n",
    "        X = activations[-1]\n",
    "        # Compute the layer parameter gradients used to update the parameters\n",
    "        grads = layer.get_params_grad(X, output_grad)\n",
    "        param_grads.appendleft(grads)\n",
    "        # Compute gradient at output of previous layer (input of current layer):\n",
    "        output_grad = input_grad\n",
    "    return list(param_grads)  # Return the parameter gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a method to update the parameters\n",
    "def update_params(layers, param_grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Function to update the parameters of the given layers with the given gradients\n",
    "    by gradient descent with the given learning rate.\n",
    "    \"\"\"\n",
    "    weights = [];\n",
    "    for layer, layer_backprop_grads in zip(layers, param_grads):\n",
    "        for param, grad in zip(layer.get_params_iter(), layer_backprop_grads):\n",
    "            # The parameter returned by the iterator point to the memory space of\n",
    "            #  the original layer and can thus be modified inplace.\n",
    "            \"\"\"\"\n",
    "            if(layer.get_layer_type() == \"linear\"):|\n",
    "                reg = 0.002* np.sum(layer.W)/layer.W.shape[1]\n",
    "                #print(np.sum(layer.W))\n",
    "            else:\n",
    "                reg = 0\n",
    "            \"\"\"\n",
    "            param -= learning_rate * grad    # Update each parameter\n",
    "        #if(layer.get_layer_type() == \"linear\"):\n",
    "            #weights.append(layer.W)\n",
    "            #print(layer.W)\n",
    "    #print_weights(\"weights\"+str(i)+\".csv\", weights);\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform backpropagation\n",
    "# initalize some lists to store the cost for future analysis       \n",
    "training_costs = []\n",
    "testing_costs= []\n",
    "minibatch_costs = []\n",
    "validation_costs= []\n",
    "\n",
    "max_nb_of_iterations = 700  # Train for a maximum of 300 iterations\n",
    "learning_rate = 0.1  # Gradient descent learning rate\n",
    "\n",
    "use_mini_batch = False\n",
    "\n",
    "# Train for the maximum number of iterations\n",
    "for iteration in range(max_nb_of_iterations):\n",
    "    #print(\"iter: \" + str(iteration))\n",
    "    if not use_mini_batch:\n",
    "        activations = forward_step(X, layers)  # Get the activations\n",
    "        param_grads = backward_step(activations, Y, layers)  # Get the gradients        \n",
    "        update_params(layers, param_grads, learning_rate)  # Update the parameters\n",
    "    else: \n",
    "        for X_batch, Y_batch in batches:  # For each minibatch sub-iteration\n",
    "            activations = forward_step(X_batch, layers)  # Get the activations\n",
    "            minibatch_cost = layers[-1].get_cost(activations[-1], Y_batch)  # Get cost\n",
    "            minibatch_costs.append(minibatch_cost)\n",
    "            param_grads = backward_step(activations, Y_batch, layers)  # Get the gradients\n",
    "            update_params(layers, param_grads, learning_rate)  # Update the parameters\n",
    "\n",
    "    \n",
    "    # Get full training cost for future analysis (plots)\n",
    "    activations = forward_step(X, layers)\n",
    "    train_cost = layers[-1].get_cost(activations[-1], Y)\n",
    "    training_costs.append(train_cost)\n",
    "    #print(\"training cost:\" + str(train_cost))\n",
    "    \n",
    "    \"\"\"\n",
    "    # Get full testing cost for future analysis (plots)\n",
    "    activations_test= forward_step(X_test, layers)\n",
    "    test_cost = layers[-1].get_cost(activations_test[-1], Y_test)\n",
    "    testing_costs.append(test_cost)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get full validation cost\n",
    "    activations = forward_step(X_validate, layers)\n",
    "    validation_cost = layers[-1].get_cost(activations[-1], Y_validate)\n",
    "    validation_costs.append(validation_cost)\n",
    "\n",
    "    if len(validation_costs) > 3:\n",
    "        # Stop training if the cost on the validation set doesn't decrease\n",
    "        #  for 3 iterations\n",
    "        if validation_costs[-1] >= validation_costs[-2] >= validation_costs[-3]:\n",
    "            print(\"not decresasing\")\n",
    "    \n",
    "nb_of_iterations = iteration + 1  # The number of iterations that have been executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREQkLf8P7+xRvMchh24AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112e49898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the cost over the iterations\n",
    "if not use_mini_batch:\n",
    "    plt.plot(training_costs, 'b-')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('$\\\\xi$', fontsize=15)\n",
    "    plt.title('Decrease of cost over backprop iteration')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "else:\n",
    "    minibatch_costs = np.array(minibatch_costs);\n",
    "    minibatch_x_inds = np.linspace(0, nb_of_iterations, num=nb_of_iterations*no_of_batches)\n",
    "    iteration_x_inds = np.linspace(1, nb_of_iterations, num=nb_of_iterations)\n",
    "    \n",
    "    # Plot the cost over the iterations\n",
    "    #plt.plot(minibatch_x_inds, minibatch_costs, 'k-', linewidth=0.5, label='cost minibatches')\n",
    "    plt.plot(iteration_x_inds, training_costs, 'r-', linewidth=2, label='cost full training set')\n",
    "    plt.plot(iteration_x_inds, validation_costs, 'b-', linewidth=3, label='cost validation set')\n",
    "    # Add labels to the plot\n",
    "    \n",
    "    #plt.plot(minibatch_costs, 'b-')\n",
    "    \n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('$\\\\xi$', fontsize=15)\n",
    "    plt.title('Decrease of cost over backprop iteration')\n",
    "    plt.legend()\n",
    "    x1,x2,y1,y2 = plt.axis()\n",
    "    plt.axis((0,nb_of_iterations,0,2.5))\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.98682024  0.01317854  0.00000121  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "X3= [-1, 1, 1, 1, -1, -1, 1, -1, 1, 1, -1, -1, 1, 1]\n",
    "X3= np.reshape(X3,  (1, 14))\n",
    "\n",
    "\n",
    "activations1 = forward_step(X3, layers)\n",
    "print(activations1.pop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 3 ..., 1 1 0]\n",
      "[2 3 3 ..., 1 1 1]\n",
      "The accuracy on the test set is 0.88\n"
     ]
    }
   ],
   "source": [
    "# Get results of test data\n",
    "y_true = np.argmax(Y_test, axis=1)  # Get the target outputs\n",
    "activations = forward_step(X_test, layers)  # Get activation of test samples\n",
    "y_pred = np.argmax(activations[-1], axis=1)  # Get the predictions made by the network\n",
    "test_accuracy = metrics.accuracy_score(y_true, y_pred)  # Test set accuracy\n",
    "print('The accuracy on the test set is {:.2f}'.format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
